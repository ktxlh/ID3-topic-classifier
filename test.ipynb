{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML HW: Decision Tree "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: UTF-8 -*-\n",
    "import logging\n",
    "import os\n",
    "import pickle\n",
    "from random import shuffle\n",
    "import gensim\n",
    "import time\n",
    "import numpy as np\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "ROOT_DIR = os.path.dirname(os.path.abspath(__file__)) # This is the Project Root\n",
    "CATEGORIES = {'证券': 0, '教育': 1, '健康': 2, '娱乐': 3, '房产': 4, '科技': 5, '财经': 6, '军事': 7, '体育': 8}\n",
    "MODEL_SIZE = 100    # TODO: tuning\n",
    "FEATURE_SPLIT = 2   # TODO: tuning\n",
    "TIME_STAMP  = time.strftime(\"%Y%m%d%H%M%S\", time.localtime())  # to avoid duplicated file names deleting files\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_files():\n",
    "    \"\"\"\n",
    "    This is one-off, facilitating cross validation.\n",
    "    Usage: shuffle_files()\n",
    "    \"\"\"\n",
    "    num_files = 0\n",
    "    for root, dirs, files in os.walk(\"new_weibo_13638/\"):\n",
    "        path = root.split(os.sep)\n",
    "        if len(files)!=0:    \n",
    "            files_copy = [f for f in files]\n",
    "            shuffle(files_copy)\n",
    "            category = CATEGORIES[os.path.basename(root)]\n",
    "            for nf, f in enumerate(files_copy):\n",
    "                num_files += 1\n",
    "                old_name = os.path.join(ROOT_DIR, root, f)\n",
    "                new_name = os.path.join(ROOT_DIR, root, f\"{category}-{nf}-{TIME_STAMP}.txt\")\n",
    "                os.rename(old_name, new_name)\n",
    "    logging.info(f\"{num_files} files shuffled.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_files(fold):\n",
    "    train_docs = []\n",
    "    test_docs = []\n",
    "\n",
    "    def read_data_from_file(root, f):\n",
    "        with open(os.path.join(ROOT_DIR, root, f), \"r\") as fin:\n",
    "            try:\n",
    "                return fin.readline().strip().split('\\t')\n",
    "            except Exception as e:\n",
    "                logging.warning(f\"Exception {e} raised by {f} in {root}\")\n",
    "                return None\n",
    "\n",
    "    # traverse root directory, and list directories as dirs and files as files\n",
    "    for root, dirs, files in os.walk(\"new_weibo_13638/\"):\n",
    "        path = root.split(os.sep)\n",
    "        if len(files)==0:\n",
    "            continue\n",
    "        #print((len(path) - 1) * '---', os.path.basename(root), len(files))\n",
    "\n",
    "        # ten-fold cross validation\n",
    "        train_cat = []\n",
    "        test_cat = []\n",
    "        for nf, f in enumerate(sorted(files)):\n",
    "\n",
    "            if nf > 15:\n",
    "                break # TODO: Take this off\n",
    "            \n",
    "            data = read_data_from_file(root, f)\n",
    "            if data:\n",
    "                if nf % 10 != fold:\n",
    "                    train_cat.append(data)\n",
    "                else:\n",
    "                    test_cat.append(data)\n",
    "        train_docs.append(train_cat)\n",
    "        test_docs.append(test_cat)\n",
    "\n",
    "\n",
    "    logging.info(f'Training data: {sum([len(cat) for cat in train_docs])} from {len(train_docs)} categories')\n",
    "    logging.info(f'Testing data: {sum([len(cat) for cat in test_docs])} from {len(test_docs)} categories')\n",
    "\n",
    "\n",
    "    #return train_docs, test_docs\n",
    "    train_texts = [doc for cat in train_docs for doc in cat]  # flattened docs\n",
    "    train_labels = [ncat for ncat,cat in enumerate(train_docs) for doc in cat]\n",
    "    test_texts = [doc for cat in test_docs for doc in cat]  # flattened docs\n",
    "    test_labels = [ncat for ncat,cat in enumerate(test_docs) for doc in cat]\n",
    "    return train_texts, train_labels, test_texts, test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_w2v(documents, save_name):\n",
    "    # build vocabulary and train model\n",
    "    model = gensim.models.Word2Vec(\n",
    "        documents,\n",
    "        size=MODEL_SIZE,\n",
    "        window=10,\n",
    "        min_count=2,\n",
    "        workers=10)\n",
    "    model.train(documents, total_examples=len(documents), epochs=10)    # TODO: tuning later (when finalizing)\n",
    "    pickle.dump(model, open(save_name,'wb'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def docs2vecs(model, docs):\n",
    "    \"\"\"\n",
    "    docs: nested list of shape (# of documents, # of words)\n",
    "    x_matrix: float64 numpy array of shape (# of documents, MODEL_SIZE)\n",
    "    \"\"\"\n",
    "    def doc2vec(doc):\n",
    "        \"\"\"\n",
    "        doc: list (# of words)\n",
    "        x_array: (MODEL_SIZE,)\n",
    "        \"\"\"\n",
    "        word_vecs = np.array([model[word] for word in doc if word in model])\n",
    "        #unk_count = sum([1 for word in doc if word not in model]) # TODO: handle <unk>\n",
    "        doc_vec = np.mean(word_vecs, 0)\n",
    "        return doc_vec\n",
    "        \n",
    "    #print(doc2vec(docs[0]).shape)\n",
    "    x_matrix = np.array([doc2vec(doc) for doc in docs])\n",
    "    return x_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "class DTreeNode:\n",
    "    def __init__(self):\n",
    "        #self.parent = parent_node\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "        self.label = None\n",
    "        \"\"\"\n",
    "        if parent_node:\n",
    "            if parent_left:\n",
    "                parent_node.left = self\n",
    "            else:\n",
    "                parent_node.right = self\n",
    "        \"\"\"\n",
    "        self.feature = None\n",
    "        self.value = None\n",
    "node = DTreeNode()\n",
    "node.value = 5\n",
    "print(node.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DTree(data, labels, features): # examples, features\n",
    "    \"\"\"\n",
    "    IN::data: np.array (# of some docs, # of all features)\n",
    "    IN::labels: list (# of some docs)\n",
    "    IN::features: a list of remaining features' indices\n",
    "    OUT::rtn: a DTreeNode\n",
    "    \"\"\"\n",
    "    def get_candidate_splits():\n",
    "        \"\"\"\n",
    "        IN::data: (# of some docs, # of all features <= MODEL_SIZE)\n",
    "        OUT::splits: (# of all features, # of splits <= FEATURE_SPLIT)\n",
    "        NOTE: Do not send data of only one doc here.\n",
    "        \"\"\"\n",
    "        num_docs = data.shape[0]\n",
    "        num_features = data.shape[1]  # for simplicity. used feature rows contain zeros\n",
    "\n",
    "        # How many rules(splits)?\n",
    "        if num_docs-1 <= FEATURE_SPLIT:\n",
    "            num_splits = num_docs-1\n",
    "        else:\n",
    "            num_splits = FEATURE_SPLIT\n",
    "\n",
    "        splits = np.zeros(shape=(num_features,num_splits))\n",
    "        for feature in features:\n",
    "            vals = np.sort(data[:,feature])\n",
    "            for i in range(num_splits):\n",
    "                split_index = int(num_docs/(num_splits+1)*(i+1))-1 \n",
    "                splits[feature,i] = (vals[split_index] + vals[split_index+1]) / 2\n",
    "        return splits\n",
    "\n",
    "    def neg_sum_entropy(feature, value):\n",
    "        \"\"\"\n",
    "        IN::feature: a feature\n",
    "        IN::value: a value of that feature\n",
    "        OUT::rtn: ∑_i ∑_j |S_i^j| log(|S_i^j|/|S_i|)\n",
    "                  where i iterates < or >=; j iterates categories\n",
    "        \"\"\"\n",
    "        s_v_c = np.zeros((2,9), dtype=np.int8)\n",
    "        for doc, label in zip(data,labels):\n",
    "            child_node = int(doc[feature] >= value)\n",
    "            s_v_c[child_node,label] += 1\n",
    "        sum_s_i = s_v_c.sum(axis=1)\n",
    "        rtn = sum([s_i_j * np.log(s_i_j/sum_s_i[i]) for i,s_i in enumerate(s_v_c) for s_i_j in s_i if s_i_j != 0 and sum_s_i[i] > 0])\n",
    "        return rtn\n",
    "\n",
    "    def get_best_rule(score_function):\n",
    "        \"\"\"\n",
    "        IN::score_function: a function accepting feature & value, returning the preference score (the higher, the better)\n",
    "        OUT::feature: int\n",
    "        OUT::value: float\n",
    "        \"\"\"\n",
    "        candidates = get_candidate_splits()\n",
    "        scores = np.array([[\n",
    "            score_function(feature, value) \n",
    "            for value in candidates[feature]] # dim 1\n",
    "            for feature in features])         # dim 0\n",
    "        a = np.max(scores, 1)    # a = features_best_values\n",
    "        b = np.argmax(a, 0)      # b = argmax_feature_index_in_scores\n",
    "        return features[b], a[b] # argmax_feature, max_value\n",
    "        \n",
    "    # Will return a DTreeNode anyway. Create one first.\n",
    "    rtn = DTreeNode()\n",
    "    \n",
    "    #If all examples are in one category, \n",
    "    #return a leaf node with that category label.\n",
    "    counted_label = [labels.count(i) for i in range(9)]\n",
    "    for label in range(9):\n",
    "        if counted_label[label] == len(labels):\n",
    "            rtn.label = label\n",
    "            return rtn\n",
    "    \n",
    "    #Else if the set of features is empty, \n",
    "    #return a leaf node with the category label \n",
    "    #that is the most common in examples. \n",
    "    most_common_label = counted_label.index(max(counted_label))\n",
    "    if len(features) == 0:\n",
    "        rtn.label = most_common_label\n",
    "        return rtn\n",
    "    \n",
    "    #Else pick a feature F and create a node R for it\n",
    "    # TODO: The same feature can be used more than once given that the splits differ\n",
    "    feature, value = get_best_rule(neg_sum_entropy)\n",
    "\n",
    "    #For each possible value vi of F: (NOTE: we have only 2: < & >=)\n",
    "    #Let examples_i be the subset of examples that have value v_i for F\n",
    "    #Add an out-going edge E to node R labeled with the value v_i.\n",
    "    rtn.feature = feature\n",
    "    rtn.value = value    \n",
    "    \n",
    "    #If examples_i is empty\n",
    "    #then attach a leaf node to edge E labeled with the category that\n",
    "    #is the most common in examples.\n",
    "    \n",
    "    #else call DTree(examplesi , features – {F}) and attach the resulting\n",
    "    #tree as the subtree under edge E.    \n",
    "    \n",
    "    lr_data = [np.zeros(shape=(0,data.shape[1])) for i in range(2)]\n",
    "    lr_labels = [[] for i in range(2)]\n",
    "    for n_doc in range(len(data)):\n",
    "        doc = data[n_doc]\n",
    "        lr = int(doc[feature] >= value)\n",
    "        lr_data[lr] = np.concatenate((lr_data[lr],[doc]))\n",
    "        lr_labels[lr].append(labels[n_doc])    \n",
    "\n",
    "    features.remove(feature)    \n",
    "\n",
    "    if len(lr_labels[0]) == 0:\n",
    "        rtn.left = DTreeNode()\n",
    "        rtn.left.label = most_common_label\n",
    "    else:\n",
    "        rtn.left = DTree(lr_data[0], lr_labels[0], features)\n",
    "        \n",
    "    if len(lr_labels[1]) == 0:\n",
    "        rtn.right = DTreeNode()\n",
    "        rtn.right.label = most_common_label\n",
    "    else:\n",
    "        rtn.right = DTree(lr_data[1], lr_labels[1], features)\n",
    "\n",
    "    #Return the subtree rooted at R.\n",
    "    return rtn\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PruneDTree(raw_tree, data, labels):\n",
    "    \"\"\"\n",
    "    IN::raw_tree: the DTree constructed\n",
    "    IN::data: np.array (# of some docs, # of all features)\n",
    "    IN::labels: list (# of some docs)\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    #Never call shuffle_files() again!!\n",
    "\n",
    "    \n",
    "    fold = 3    # TODO: fold - from 0 to 9\n",
    "    train_texts, train_y, test_texts, test_y = load_files(fold)\n",
    "    \n",
    "    # compute attr\n",
    "    ## train word2vec\n",
    "    model_name = f'model/model-{fold}.w2v'\n",
    "    #model = train_w2v(train_texts, model_name)\n",
    "    model = pickle.load(open(model_name,'rb'))\n",
    "    \n",
    "    ### tag words & docs\n",
    "    train_x = docs2vecs(model, train_texts)\n",
    "    test_x = docs2vecs(model, test_texts)\n",
    "    \n",
    "    \n",
    "    # decision tree\n",
    "    ## impurity function\n",
    "    ### -[*] Entropy\n",
    "    ### -[ ] Gini index\n",
    "    ### -[ ] Misclassification error\n",
    "\n",
    "    ## growing\n",
    "    ### if termination condition not reached\n",
    "    ### calculate each candidate's info gain\n",
    "    ### choose the best one and split\n",
    "    ### recursion\n",
    "    raw_tree = DTree(train_x, train_y, [i for i in range(train_x.shape[1])])\n",
    "    # NOTE: To pickle a self-defined type (e.g. DTreeNode),\n",
    "    # see https://stackoverflow.com/questions/27351980/how-to-add-a-custom-type-to-dills-pickleable-types\n",
    "\n",
    "    ## post-pruning\n",
    "    pruned_tree = PruneDTree(raw_tree, train_x, train_y)\n",
    "\n",
    "    # Testing\n",
    "    ## accuracy\n",
    "    ## f-measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2. 3.]\n"
     ]
    }
   ],
   "source": [
    "word_vecs = np.array([[0,1],[2,3],[4,5]])\n",
    "#unk_count = sum([1 for word in doc if word not in model]) # TODO: handle <unk>\n",
    "doc_vec = np.mean(word_vecs, 0)\n",
    "print(doc_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0"
      ]
     },
     "execution_count": 360,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [1,2,3]\n",
    "np.mean(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.38629436  0.         -1.38629436]\n",
      " [-1.38629436  0.         -1.38629436]]\n",
      "(0, 1.9)\n"
     ]
    }
   ],
   "source": [
    "data = np.array([[1,2],[3,4]])\n",
    "labels = [0,1]\n",
    "def neg_sum_entropy(feature, value):\n",
    "    \"\"\"\n",
    "    IN::feature: a feature\n",
    "    IN::value: a value of that feature\n",
    "    OUT::rtn: ∑_i ∑_j |S_i^j| log(|S_i^j|/|S_i|)\n",
    "              where i iterates < or >=; j iterates categories\n",
    "    \"\"\"\n",
    "    s_v_c = np.zeros((2,9), dtype=np.int8)\n",
    "    for doc, label in zip(data,labels):\n",
    "        child_node = int(doc[feature] >= value)\n",
    "        s_v_c[child_node,label] += 1\n",
    "    sum_s_i = s_v_c.sum(axis=1)\n",
    "    rtn = sum([s_i_j * np.log(s_i_j/sum_s_i[i]) for i,s_i in enumerate(s_v_c) for s_i_j in s_i if s_i_j != 0 and sum_s_i[i] > 0])\n",
    "    return rtn\n",
    "\n",
    "def get_best_rule(score_function):\n",
    "    \"\"\"\n",
    "    IN::score_function: a function accepting feature & value, returning the preference score (the higher, the better)\n",
    "    OUT::feature: int\n",
    "    OUT::value: float\n",
    "    \"\"\"\n",
    "    #candidates = get_candidate_splits()\n",
    "    features = [0,1]\n",
    "    candidates = np.array([[0.3,1.9,3.5],[1.0,2.4,4.3]])\n",
    "    \n",
    "    scores = np.array([[\n",
    "        score_function(feature, value) \n",
    "        for value in candidates[feature]] # dim 1\n",
    "        for feature in features])         # dim 0\n",
    "    print(scores)\n",
    "    a = np.max(scores, axis=1)       # a = features best values\n",
    "    b = np.argmax(a, axis=0)         # b = argmax feature index in scoress\n",
    "    c = np.argmax(scores[b], axis=0) # c = argmax value index of the feature\n",
    "    return features[b], candidates[features[b],c]   # argmax feature index, argmax value\n",
    "\n",
    "print(get_best_rule(neg_sum_entropy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "10\n",
      "0.5\n"
     ]
    }
   ],
   "source": [
    "data = np.array([[1,2],[3,4]])\n",
    "print(data.trace())\n",
    "print(data.sum())\n",
    "print(data.trace()/data.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/anaconda3/bin:/anaconda3/bin:/usr/bin:/bin:/usr/sbin:/sbin\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.environ['PATH'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
